{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importer les bibliothèques nécessaires\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Input\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import load_model\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_number</th>\n",
       "      <th>datetime</th>\n",
       "      <th>Consommation</th>\n",
       "      <th>cluster</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2016-03-01 01:19:43</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2016-03-01 02:19:43</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>2016-03-01 03:19:43</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>2016-03-01 04:19:43</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>2016-03-01 05:39:54</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>673922</th>\n",
       "      <td>92</td>\n",
       "      <td>2017-02-28 18:08:01</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>673923</th>\n",
       "      <td>92</td>\n",
       "      <td>2017-02-28 19:08:06</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>673924</th>\n",
       "      <td>92</td>\n",
       "      <td>2017-02-28 20:08:06</td>\n",
       "      <td>27</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>673925</th>\n",
       "      <td>92</td>\n",
       "      <td>2017-02-28 21:08:06</td>\n",
       "      <td>43</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>673926</th>\n",
       "      <td>92</td>\n",
       "      <td>2017-02-28 22:08:06</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>673927 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        user_number             datetime  Consommation  cluster\n",
       "0                 1  2016-03-01 01:19:43             0        0\n",
       "1                 1  2016-03-01 02:19:43             0        0\n",
       "2                 1  2016-03-01 03:19:43             0        0\n",
       "3                 1  2016-03-01 04:19:43             0        0\n",
       "4                 1  2016-03-01 05:39:54             0        0\n",
       "...             ...                  ...           ...      ...\n",
       "673922           92  2017-02-28 18:08:01             0        2\n",
       "673923           92  2017-02-28 19:08:06             0        2\n",
       "673924           92  2017-02-28 20:08:06            27        2\n",
       "673925           92  2017-02-28 21:08:06            43        2\n",
       "673926           92  2017-02-28 22:08:06             5        2\n",
       "\n",
       "[673927 rows x 4 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path = \"df_with_clusters.csv\"  \n",
    "df = pd.read_csv(file_path)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entraînement de l'autoencoder pour le cluster 0\n",
      "Epoch 1/50\n",
      "\u001b[1m7267/7267\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 1ms/step - loss: 3.6926e-05 - val_loss: 7.4323e-07\n",
      "Epoch 2/50\n",
      "\u001b[1m7267/7267\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 1ms/step - loss: 1.7614e-06 - val_loss: 7.2063e-07\n",
      "Epoch 3/50\n",
      "\u001b[1m7267/7267\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 1ms/step - loss: 4.4259e-06 - val_loss: 7.3571e-07\n",
      "Epoch 4/50\n",
      "\u001b[1m7267/7267\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 1ms/step - loss: 4.8858e-06 - val_loss: 7.0733e-07\n",
      "Epoch 5/50\n",
      "\u001b[1m7267/7267\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 1ms/step - loss: 2.0623e-06 - val_loss: 5.6905e-07\n",
      "Epoch 6/50\n",
      "\u001b[1m7267/7267\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 1ms/step - loss: 2.1599e-06 - val_loss: 5.4448e-07\n",
      "Epoch 7/50\n",
      "\u001b[1m7267/7267\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 2ms/step - loss: 2.8923e-06 - val_loss: 4.7651e-07\n",
      "Epoch 8/50\n",
      "\u001b[1m7267/7267\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 2ms/step - loss: 2.4781e-06 - val_loss: 4.3758e-07\n",
      "Epoch 9/50\n",
      "\u001b[1m7267/7267\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 2ms/step - loss: 2.3760e-06 - val_loss: 4.5562e-07\n",
      "Epoch 10/50\n",
      "\u001b[1m7267/7267\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 2ms/step - loss: 1.9615e-06 - val_loss: 3.4091e-07\n",
      "Epoch 11/50\n",
      "\u001b[1m7267/7267\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 2ms/step - loss: 1.1247e-06 - val_loss: 2.8905e-07\n",
      "Epoch 12/50\n",
      "\u001b[1m7267/7267\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 2ms/step - loss: 2.1337e-06 - val_loss: 2.4847e-07\n",
      "Epoch 13/50\n",
      "\u001b[1m7267/7267\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 2ms/step - loss: 5.8888e-07 - val_loss: 2.0310e-07\n",
      "Epoch 14/50\n",
      "\u001b[1m7267/7267\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 3ms/step - loss: 1.0060e-06 - val_loss: 2.0782e-07\n",
      "Epoch 15/50\n",
      "\u001b[1m7267/7267\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 2ms/step - loss: 6.1216e-07 - val_loss: 1.5520e-07\n",
      "Epoch 16/50\n",
      "\u001b[1m7267/7267\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 2ms/step - loss: 1.1225e-06 - val_loss: 1.0267e-07\n",
      "Epoch 17/50\n",
      "\u001b[1m7267/7267\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 3ms/step - loss: 5.3801e-07 - val_loss: 8.2538e-08\n",
      "Epoch 18/50\n",
      "\u001b[1m7267/7267\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 3ms/step - loss: 9.3761e-07 - val_loss: 5.6819e-08\n",
      "Epoch 19/50\n",
      "\u001b[1m7267/7267\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 2ms/step - loss: 2.3658e-07 - val_loss: 3.5732e-08\n",
      "Epoch 20/50\n",
      "\u001b[1m7267/7267\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 2ms/step - loss: 2.5657e-07 - val_loss: 3.0716e-08\n",
      "Epoch 21/50\n",
      "\u001b[1m7267/7267\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 2ms/step - loss: 4.2999e-07 - val_loss: 2.0754e-08\n",
      "Epoch 22/50\n",
      "\u001b[1m7267/7267\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 2ms/step - loss: 9.8744e-08 - val_loss: 4.7216e-09\n",
      "Epoch 23/50\n",
      "\u001b[1m7267/7267\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 2ms/step - loss: 3.5523e-07 - val_loss: 1.9828e-09\n",
      "Epoch 24/50\n",
      "\u001b[1m7267/7267\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 2ms/step - loss: 6.1567e-07 - val_loss: 7.9122e-10\n",
      "Epoch 25/50\n",
      "\u001b[1m7267/7267\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 2ms/step - loss: 2.1268e-07 - val_loss: 7.3793e-09\n",
      "Epoch 26/50\n",
      "\u001b[1m7267/7267\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 2ms/step - loss: 1.9856e-07 - val_loss: 8.6774e-10\n",
      "Epoch 27/50\n",
      "\u001b[1m7267/7267\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 2ms/step - loss: 1.2782e-07 - val_loss: 3.6223e-09\n",
      "Epoch 28/50\n",
      "\u001b[1m7267/7267\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 2ms/step - loss: 1.1124e-07 - val_loss: 6.7719e-09\n",
      "Epoch 29/50\n",
      "\u001b[1m7267/7267\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 2ms/step - loss: 3.2548e-08 - val_loss: 3.6205e-09\n",
      "Epoch 30/50\n",
      "\u001b[1m7267/7267\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 2ms/step - loss: 2.8318e-07 - val_loss: 4.3188e-09\n",
      "Epoch 31/50\n",
      "\u001b[1m7267/7267\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 2ms/step - loss: 4.7029e-08 - val_loss: 1.0398e-08\n",
      "Epoch 32/50\n",
      "\u001b[1m7267/7267\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 2ms/step - loss: 1.4886e-07 - val_loss: 7.7652e-09\n",
      "Epoch 33/50\n",
      "\u001b[1m7267/7267\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 2ms/step - loss: 5.8131e-08 - val_loss: 2.9944e-08\n",
      "Epoch 34/50\n",
      "\u001b[1m7267/7267\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 2ms/step - loss: 7.2119e-08 - val_loss: 1.1589e-08\n",
      "Epoch 35/50\n",
      "\u001b[1m7267/7267\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 2ms/step - loss: 7.2982e-08 - val_loss: 1.2349e-08\n",
      "Epoch 36/50\n",
      "\u001b[1m7267/7267\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 2ms/step - loss: 1.9468e-07 - val_loss: 9.3436e-09\n",
      "Epoch 37/50\n",
      "\u001b[1m7267/7267\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 2ms/step - loss: 6.5575e-08 - val_loss: 1.4722e-08\n",
      "Epoch 38/50\n",
      "\u001b[1m7267/7267\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 2ms/step - loss: 2.3822e-07 - val_loss: 9.1632e-09\n",
      "Epoch 39/50\n",
      "\u001b[1m7267/7267\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 2ms/step - loss: 9.0947e-08 - val_loss: 1.0460e-08\n",
      "Epoch 40/50\n",
      "\u001b[1m7267/7267\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 2ms/step - loss: 2.0057e-07 - val_loss: 1.0005e-08\n",
      "Epoch 41/50\n",
      "\u001b[1m7267/7267\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 2ms/step - loss: 1.3734e-07 - val_loss: 1.1851e-08\n",
      "Epoch 42/50\n",
      "\u001b[1m7267/7267\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 2ms/step - loss: 5.8456e-08 - val_loss: 9.4246e-09\n",
      "Epoch 43/50\n",
      "\u001b[1m7267/7267\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 2ms/step - loss: 1.0575e-07 - val_loss: 1.5040e-08\n",
      "Epoch 44/50\n",
      "\u001b[1m7267/7267\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 2ms/step - loss: 4.8002e-08 - val_loss: 9.8842e-09\n",
      "Epoch 45/50\n",
      "\u001b[1m7267/7267\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 2ms/step - loss: 1.4539e-07 - val_loss: 1.2307e-08\n",
      "Epoch 46/50\n",
      "\u001b[1m7267/7267\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 2ms/step - loss: 5.1812e-08 - val_loss: 1.3587e-08\n",
      "Epoch 47/50\n",
      "\u001b[1m7267/7267\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 2ms/step - loss: 3.2683e-07 - val_loss: 1.5067e-08\n",
      "Epoch 48/50\n",
      "\u001b[1m7267/7267\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 2ms/step - loss: 8.8012e-08 - val_loss: 4.1571e-08\n",
      "Epoch 49/50\n",
      "\u001b[1m7267/7267\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 2ms/step - loss: 3.6267e-08 - val_loss: 1.2235e-08\n",
      "Epoch 50/50\n",
      "\u001b[1m7267/7267\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 2ms/step - loss: 1.7461e-07 - val_loss: 1.1778e-08\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modèle du cluster 0 sauvegardé sous autoencoder_models/autoencoder_cluster_0.h5\n",
      "Entraînement de l'autoencoder pour le cluster 1\n",
      "Epoch 1/50\n",
      "\u001b[1m2368/2368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2ms/step - loss: 0.0916 - val_loss: 7.8041e-04\n",
      "Epoch 2/50\n",
      "\u001b[1m2368/2368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - loss: 4.8901e-04 - val_loss: 9.6044e-05\n",
      "Epoch 3/50\n",
      "\u001b[1m2368/2368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - loss: 9.5811e-05 - val_loss: 1.9708e-05\n",
      "Epoch 4/50\n",
      "\u001b[1m2368/2368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - loss: 1.6841e-05 - val_loss: 6.2698e-06\n",
      "Epoch 5/50\n",
      "\u001b[1m2368/2368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - loss: 4.2021e-05 - val_loss: 4.0972e-06\n",
      "Epoch 6/50\n",
      "\u001b[1m2368/2368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - loss: 2.2457e-05 - val_loss: 3.9179e-06\n",
      "Epoch 7/50\n",
      "\u001b[1m2368/2368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - loss: 2.6013e-05 - val_loss: 3.9134e-06\n",
      "Epoch 8/50\n",
      "\u001b[1m2368/2368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2ms/step - loss: 6.8738e-06 - val_loss: 3.9105e-06\n",
      "Epoch 9/50\n",
      "\u001b[1m2368/2368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2ms/step - loss: 2.1339e-05 - val_loss: 3.9061e-06\n",
      "Epoch 10/50\n",
      "\u001b[1m2368/2368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - loss: 2.2587e-05 - val_loss: 3.9014e-06\n",
      "Epoch 11/50\n",
      "\u001b[1m2368/2368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - loss: 1.7712e-05 - val_loss: 3.8965e-06\n",
      "Epoch 12/50\n",
      "\u001b[1m2368/2368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - loss: 1.2138e-05 - val_loss: 3.8902e-06\n",
      "Epoch 13/50\n",
      "\u001b[1m2368/2368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - loss: 1.1125e-05 - val_loss: 3.8842e-06\n",
      "Epoch 14/50\n",
      "\u001b[1m2368/2368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - loss: 1.2697e-05 - val_loss: 3.8790e-06\n",
      "Epoch 15/50\n",
      "\u001b[1m2368/2368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - loss: 5.6994e-05 - val_loss: 3.8709e-06\n",
      "Epoch 16/50\n",
      "\u001b[1m2368/2368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - loss: 4.0053e-05 - val_loss: 3.8632e-06\n",
      "Epoch 17/50\n",
      "\u001b[1m2368/2368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - loss: 3.7771e-06 - val_loss: 4.1023e-06\n",
      "Epoch 18/50\n",
      "\u001b[1m2368/2368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - loss: 6.7960e-06 - val_loss: 3.8493e-06\n",
      "Epoch 19/50\n",
      "\u001b[1m2368/2368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - loss: 1.0941e-05 - val_loss: 3.8376e-06\n",
      "Epoch 20/50\n",
      "\u001b[1m2368/2368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - loss: 2.3088e-05 - val_loss: 3.8322e-06\n",
      "Epoch 21/50\n",
      "\u001b[1m2368/2368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - loss: 1.0259e-05 - val_loss: 3.8236e-06\n",
      "Epoch 22/50\n",
      "\u001b[1m2368/2368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - loss: 1.3164e-05 - val_loss: 3.8210e-06\n",
      "Epoch 23/50\n",
      "\u001b[1m2368/2368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - loss: 1.4258e-05 - val_loss: 3.8157e-06\n",
      "Epoch 24/50\n",
      "\u001b[1m2368/2368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - loss: 8.6343e-06 - val_loss: 3.8698e-06\n",
      "Epoch 25/50\n",
      "\u001b[1m2368/2368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - loss: 1.8445e-05 - val_loss: 3.8200e-06\n",
      "Epoch 26/50\n",
      "\u001b[1m2368/2368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - loss: 1.0469e-05 - val_loss: 4.0550e-06\n",
      "Epoch 27/50\n",
      "\u001b[1m2368/2368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - loss: 2.6475e-05 - val_loss: 3.8305e-06\n",
      "Epoch 28/50\n",
      "\u001b[1m2368/2368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - loss: 1.1510e-05 - val_loss: 4.0805e-06\n",
      "Epoch 29/50\n",
      "\u001b[1m2368/2368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - loss: 1.8688e-05 - val_loss: 3.8178e-06\n",
      "Epoch 30/50\n",
      "\u001b[1m2368/2368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - loss: 1.9336e-05 - val_loss: 3.8676e-06\n",
      "Epoch 31/50\n",
      "\u001b[1m2368/2368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - loss: 3.6207e-06 - val_loss: 4.5141e-06\n",
      "Epoch 32/50\n",
      "\u001b[1m2368/2368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - loss: 4.3379e-06 - val_loss: 3.9178e-06\n",
      "Epoch 33/50\n",
      "\u001b[1m2368/2368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - loss: 3.9853e-06 - val_loss: 3.8664e-06\n",
      "Epoch 34/50\n",
      "\u001b[1m2368/2368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - loss: 2.5390e-06 - val_loss: 3.7533e-06\n",
      "Epoch 35/50\n",
      "\u001b[1m2368/2368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - loss: 7.6106e-06 - val_loss: 3.6013e-06\n",
      "Epoch 36/50\n",
      "\u001b[1m2368/2368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - loss: 5.7173e-06 - val_loss: 3.5565e-06\n",
      "Epoch 37/50\n",
      "\u001b[1m2368/2368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - loss: 2.1431e-06 - val_loss: 3.5263e-06\n",
      "Epoch 38/50\n",
      "\u001b[1m2368/2368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - loss: 1.3863e-06 - val_loss: 3.7615e-06\n",
      "Epoch 39/50\n",
      "\u001b[1m2368/2368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - loss: 2.5284e-06 - val_loss: 3.4117e-06\n",
      "Epoch 40/50\n",
      "\u001b[1m2368/2368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - loss: 1.9770e-06 - val_loss: 3.3539e-06\n",
      "Epoch 41/50\n",
      "\u001b[1m2368/2368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - loss: 1.5674e-06 - val_loss: 3.2856e-06\n",
      "Epoch 42/50\n",
      "\u001b[1m2368/2368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - loss: 1.6117e-06 - val_loss: 3.2189e-06\n",
      "Epoch 43/50\n",
      "\u001b[1m2368/2368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - loss: 1.4152e-06 - val_loss: 3.2022e-06\n",
      "Epoch 44/50\n",
      "\u001b[1m2368/2368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - loss: 2.4724e-06 - val_loss: 3.1976e-06\n",
      "Epoch 45/50\n",
      "\u001b[1m2368/2368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - loss: 1.0568e-06 - val_loss: 3.4660e-06\n",
      "Epoch 46/50\n",
      "\u001b[1m2368/2368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - loss: 2.1479e-06 - val_loss: 3.1452e-06\n",
      "Epoch 47/50\n",
      "\u001b[1m2368/2368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - loss: 1.9342e-06 - val_loss: 3.1639e-06\n",
      "Epoch 48/50\n",
      "\u001b[1m2368/2368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - loss: 1.6066e-06 - val_loss: 3.3048e-06\n",
      "Epoch 49/50\n",
      "\u001b[1m2368/2368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - loss: 2.7428e-06 - val_loss: 3.4861e-06\n",
      "Epoch 50/50\n",
      "\u001b[1m2368/2368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - loss: 1.5270e-06 - val_loss: 3.3587e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modèle du cluster 1 sauvegardé sous autoencoder_models/autoencoder_cluster_1.h5\n",
      "Entraînement de l'autoencoder pour le cluster 2\n",
      "Epoch 1/50\n",
      "\u001b[1m7214/7214\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 2ms/step - loss: 3.3670e-06 - val_loss: 8.0684e-06\n",
      "Epoch 2/50\n",
      "\u001b[1m7214/7214\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 2ms/step - loss: 5.1648e-06 - val_loss: 7.2325e-06\n",
      "Epoch 3/50\n",
      "\u001b[1m7214/7214\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 2ms/step - loss: 1.0286e-05 - val_loss: 6.2581e-06\n",
      "Epoch 4/50\n",
      "\u001b[1m7214/7214\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 2ms/step - loss: 3.1954e-06 - val_loss: 5.6655e-06\n",
      "Epoch 5/50\n",
      "\u001b[1m7214/7214\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 2ms/step - loss: 3.6809e-06 - val_loss: 4.9813e-06\n",
      "Epoch 6/50\n",
      "\u001b[1m7214/7214\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 2ms/step - loss: 7.0539e-06 - val_loss: 4.4928e-06\n",
      "Epoch 7/50\n",
      "\u001b[1m7214/7214\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 2ms/step - loss: 2.7668e-06 - val_loss: 3.7079e-06\n",
      "Epoch 8/50\n",
      "\u001b[1m7214/7214\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 2ms/step - loss: 1.8955e-06 - val_loss: 3.1994e-06\n",
      "Epoch 9/50\n",
      "\u001b[1m7214/7214\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 2ms/step - loss: 1.9911e-06 - val_loss: 2.7042e-06\n",
      "Epoch 10/50\n",
      "\u001b[1m7214/7214\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 2ms/step - loss: 2.0111e-06 - val_loss: 2.3438e-06\n",
      "Epoch 11/50\n",
      "\u001b[1m7214/7214\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 2ms/step - loss: 1.5591e-06 - val_loss: 2.0474e-06\n",
      "Epoch 12/50\n",
      "\u001b[1m7214/7214\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 2ms/step - loss: 1.2406e-06 - val_loss: 1.6947e-06\n",
      "Epoch 13/50\n",
      "\u001b[1m7214/7214\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 2ms/step - loss: 6.3810e-07 - val_loss: 1.4607e-06\n",
      "Epoch 14/50\n",
      "\u001b[1m7214/7214\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 2ms/step - loss: 2.2430e-06 - val_loss: 1.1561e-06\n",
      "Epoch 15/50\n",
      "\u001b[1m7214/7214\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 2ms/step - loss: 4.0783e-07 - val_loss: 1.0062e-06\n",
      "Epoch 16/50\n",
      "\u001b[1m7214/7214\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 2ms/step - loss: 7.5981e-07 - val_loss: 8.4794e-07\n",
      "Epoch 17/50\n",
      "\u001b[1m7214/7214\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 2ms/step - loss: 8.0917e-07 - val_loss: 6.9903e-07\n",
      "Epoch 18/50\n",
      "\u001b[1m7214/7214\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 2ms/step - loss: 4.1347e-07 - val_loss: 6.0723e-07\n",
      "Epoch 19/50\n",
      "\u001b[1m7214/7214\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 2ms/step - loss: 4.2235e-07 - val_loss: 5.2399e-07\n",
      "Epoch 20/50\n",
      "\u001b[1m7214/7214\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 2ms/step - loss: 4.6156e-07 - val_loss: 4.4929e-07\n",
      "Epoch 21/50\n",
      "\u001b[1m7214/7214\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 2ms/step - loss: 3.8679e-07 - val_loss: 3.9297e-07\n",
      "Epoch 22/50\n",
      "\u001b[1m7214/7214\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 2ms/step - loss: 2.9477e-07 - val_loss: 3.3300e-07\n",
      "Epoch 23/50\n",
      "\u001b[1m7214/7214\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 2ms/step - loss: 2.4568e-07 - val_loss: 2.9050e-07\n",
      "Epoch 24/50\n",
      "\u001b[1m7214/7214\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 2ms/step - loss: 2.5758e-07 - val_loss: 3.1433e-07\n",
      "Epoch 25/50\n",
      "\u001b[1m7214/7214\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 2ms/step - loss: 2.0906e-07 - val_loss: 2.2869e-07\n",
      "Epoch 26/50\n",
      "\u001b[1m7214/7214\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 2ms/step - loss: 1.1543e-07 - val_loss: 2.0608e-07\n",
      "Epoch 27/50\n",
      "\u001b[1m7214/7214\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 2ms/step - loss: 1.9479e-07 - val_loss: 1.8807e-07\n",
      "Epoch 28/50\n",
      "\u001b[1m7214/7214\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 2ms/step - loss: 1.0832e-07 - val_loss: 1.7059e-07\n",
      "Epoch 29/50\n",
      "\u001b[1m7214/7214\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 2ms/step - loss: 7.6800e-08 - val_loss: 1.5712e-07\n",
      "Epoch 30/50\n",
      "\u001b[1m7214/7214\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 2ms/step - loss: 2.4313e-07 - val_loss: 1.4368e-07\n",
      "Epoch 31/50\n",
      "\u001b[1m7214/7214\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 2ms/step - loss: 1.8633e-07 - val_loss: 1.3386e-07\n",
      "Epoch 32/50\n",
      "\u001b[1m7214/7214\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 2ms/step - loss: 9.8648e-08 - val_loss: 1.2583e-07\n",
      "Epoch 33/50\n",
      "\u001b[1m7214/7214\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 2ms/step - loss: 1.4942e-07 - val_loss: 1.1439e-07\n",
      "Epoch 34/50\n",
      "\u001b[1m7214/7214\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 2ms/step - loss: 7.7925e-08 - val_loss: 1.0732e-07\n",
      "Epoch 35/50\n",
      "\u001b[1m7214/7214\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 2ms/step - loss: 1.5539e-07 - val_loss: 9.8685e-08\n",
      "Epoch 36/50\n",
      "\u001b[1m7214/7214\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 2ms/step - loss: 6.0066e-08 - val_loss: 1.0334e-07\n",
      "Epoch 37/50\n",
      "\u001b[1m7214/7214\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 2ms/step - loss: 1.1674e-07 - val_loss: 8.7260e-08\n",
      "Epoch 38/50\n",
      "\u001b[1m7214/7214\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 2ms/step - loss: 5.7375e-08 - val_loss: 8.2774e-08\n",
      "Epoch 39/50\n",
      "\u001b[1m7214/7214\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 2ms/step - loss: 6.7771e-08 - val_loss: 8.5891e-08\n",
      "Epoch 40/50\n",
      "\u001b[1m7214/7214\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 2ms/step - loss: 4.6650e-08 - val_loss: 7.9862e-08\n",
      "Epoch 41/50\n",
      "\u001b[1m7214/7214\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 2ms/step - loss: 4.5180e-08 - val_loss: 7.2857e-08\n",
      "Epoch 42/50\n",
      "\u001b[1m7214/7214\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 2ms/step - loss: 6.2270e-08 - val_loss: 6.8037e-08\n",
      "Epoch 43/50\n",
      "\u001b[1m7214/7214\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 2ms/step - loss: 6.0829e-08 - val_loss: 6.4194e-08\n",
      "Epoch 44/50\n",
      "\u001b[1m7214/7214\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 2ms/step - loss: 4.3394e-08 - val_loss: 6.5128e-08\n",
      "Epoch 45/50\n",
      "\u001b[1m7214/7214\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 2ms/step - loss: 6.0628e-08 - val_loss: 5.8804e-08\n",
      "Epoch 46/50\n",
      "\u001b[1m7214/7214\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 2ms/step - loss: 7.4272e-08 - val_loss: 5.6682e-08\n",
      "Epoch 47/50\n",
      "\u001b[1m7214/7214\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 2ms/step - loss: 7.2940e-08 - val_loss: 6.7145e-08\n",
      "Epoch 48/50\n",
      "\u001b[1m7214/7214\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 2ms/step - loss: 5.4306e-08 - val_loss: 5.3663e-08\n",
      "Epoch 49/50\n",
      "\u001b[1m7214/7214\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 2ms/step - loss: 4.9148e-08 - val_loss: 5.3200e-08\n",
      "Epoch 50/50\n",
      "\u001b[1m7214/7214\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 2ms/step - loss: 5.6862e-08 - val_loss: 4.9299e-08\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modèle du cluster 2 sauvegardé sous autoencoder_models/autoencoder_cluster_2.h5\n"
     ]
    }
   ],
   "source": [
    "# Séparer les utilisateurs par cluster\n",
    "clusters = df['cluster'].unique()\n",
    "\n",
    "# Créer un répertoire pour enregistrer les modèles\n",
    "os.makedirs(\"autoencoder_models\", exist_ok=True)\n",
    "\n",
    "# Fonction pour entraîner un autoencoder pour chaque cluster et sauvegarder le modèle\n",
    "def train_and_save_autoencoder(data, cluster_id):\n",
    "    print(f\"Entraînement de l'autoencoder pour le cluster {cluster_id}\")\n",
    "    \n",
    "    # Extraire les données de consommation pour les utilisateurs dans ce cluster\n",
    "    cluster_data = data[data['cluster'] == cluster_id]\n",
    "    \n",
    "    # Sélectionner les colonnes pertinentes pour l'entraînement\n",
    "    cluster_data = cluster_data[['Consommation']]  # Vous pouvez ajouter d'autres colonnes si nécessaire\n",
    "    \n",
    "    # Normaliser les données de consommation\n",
    "    scaler = MinMaxScaler()\n",
    "    data_scaled = scaler.fit_transform(cluster_data)\n",
    "    \n",
    "    # Diviser les données en ensemble d'entraînement et de validation\n",
    "    X_train, X_val = train_test_split(data_scaled, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Définir l'architecture de l'autoencoder\n",
    "    input_dim = X_train.shape[1]  # Nombre de caractéristiques d'entrée (une seule pour 'Consommation')\n",
    "    encoding_dim = 8  # Nombre de neurones dans la couche cachée (dimension réduite)\n",
    "\n",
    "    # Définir les couches de l'autoencoder\n",
    "    input_layer = Input(shape=(input_dim,))\n",
    "    encoded = Dense(encoding_dim, activation='relu')(input_layer)\n",
    "    decoded = Dense(input_dim, activation='sigmoid')(encoded)\n",
    "\n",
    "    # Construire le modèle autoencoder\n",
    "    autoencoder = Model(inputs=input_layer, outputs=decoded)\n",
    "\n",
    "    # Compiler le modèle\n",
    "    autoencoder.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "    # Entraîner le modèle\n",
    "    autoencoder.fit(X_train, X_train, epochs=50, batch_size=32, validation_data=(X_val, X_val), shuffle=True)\n",
    "\n",
    "    # Sauvegarder le modèle\n",
    "    model_filename = f\"autoencoder_models/autoencoder_cluster_{cluster_id}.h5\"\n",
    "    autoencoder.save(model_filename)\n",
    "    print(f\"Modèle du cluster {cluster_id} sauvegardé sous {model_filename}\")\n",
    "    \n",
    "# Boucle à travers les clusters pour entraîner un autoencoder pour chaque cluster et sauvegarder les modèles\n",
    "for cluster_id in clusters:\n",
    "    train_and_save_autoencoder(df, cluster_id)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
